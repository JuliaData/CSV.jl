<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples · CSV.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CSV.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Examples</a><ul class="internal"><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Examples</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaData/CSV.jl/blob/master/docs/src/examples.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><ul><ul><li><a href="#API-Reference">API Reference</a></li><li><a href="#Utilities">Utilities</a></li><li><a href="#Examples">Examples</a></li><ul><li><a href="#stringencodings">Non-UTF-8 character encodings</a></li><li><a href="#gzipped_input">Gzipped input</a></li><li><a href="#csv_string">Delimited data in a string</a></li><li><a href="#http">Data from the web/a url</a></li><li><a href="#second_row_header">Column names on 2nd row</a></li><li><a href="#no_header">No column names in data</a></li><li><a href="#manual_header">Manually provide column names</a></li><li><a href="#multi_row_header">Multi-row column names</a></li><li><a href="#normalize_header">Normalizing column names</a></li><li><a href="#skipto_example">Skip to specific row where data starts</a></li><li><a href="#footerskip_example">Skipping trailing useless rows</a></li><li><a href="#transpose_example">Reading transposed data</a></li><li><a href="#comment_example">Ignoring commented rows</a></li><li><a href="#ignoreemptyrows_example">Ignoring empty rows</a></li><li><a href="#select_example">Including/excluding columns</a></li><li><a href="#limit_example">Limiting number of rows from data</a></li><li><a href="#missing_string_example">Specifying custom missing strings</a></li><li><a href="#string_delim">String delimiter</a></li><li><a href="#ignorerepeated_example">Fixed width files</a></li><li><a href="#quoted_example">Turning off quoted cell parsing</a></li><li><a href="#quotechar_example">Quoted &amp; escaped fields</a></li><li><a href="#dateformat_example">DateFormat</a></li><li><a href="#decimal_example">Custom decimal separator</a></li><li><a href="#truestrings_example">Custom bool strings</a></li><li><a href="#matrix_example">Matrix-like Data</a></li><li><a href="#types_example">Providing types</a></li><li><a href="#typemap_example">Typemap</a></li><li><a href="#pool_example">Pooled values</a></li></ul></ul><li><a href="../#CSV.jl-Documentation">CSV.jl Documentation</a></li><li><a href="../#Reading">Reading</a></li><ul><li><a href="../#input"><code>input</code></a></li><ul><li><a href="../#Examples">Examples</a></li></ul><li><a href="../#header"><code>header</code></a></li><ul><li><a href="../#Examples-2">Examples</a></li></ul><li><a href="../#normalizenames"><code>normalizenames</code></a></li><ul><li><a href="../#Examples-3">Examples</a></li></ul><li><a href="../#skipto"><code>skipto</code></a></li><ul><li><a href="../#Examples-4">Examples</a></li></ul><li><a href="../#footerskip"><code>footerskip</code></a></li><ul><li><a href="../#Examples-5">Examples</a></li></ul><li><a href="../#transpose"><code>transpose</code></a></li><ul><li><a href="../#Examples-6">Examples</a></li></ul><li><a href="../#comment"><code>comment</code></a></li><ul><li><a href="../#Examples-7">Examples</a></li></ul><li><a href="../#ignoreemptyrows"><code>ignoreemptyrows</code></a></li><ul><li><a href="../#Examples-8">Examples</a></li></ul><li><a href="../#select"><code>select</code> / <code>drop</code></a></li><ul><li><a href="../#Examples-9">Examples</a></li></ul><li><a href="../#limit"><code>limit</code></a></li><ul><li><a href="../#Examples-10">Examples</a></li></ul><li><a href="../#ntasks"><code>ntasks</code></a></li><li><a href="../#rows_to_check"><code>rows_to_check</code></a></li><li><a href="../#missingstring"><code>missingstring</code></a></li><ul><li><a href="../#Examples-11">Examples</a></li></ul><li><a href="../#delim"><code>delim</code></a></li><ul><li><a href="../#Examples-12">Examples</a></li></ul><li><a href="../#ignorerepeated"><code>ignorerepeated</code></a></li><ul><li><a href="../#Examples-13">Examples</a></li></ul><li><a href="../#quoted"><code>quoted</code></a></li><ul><li><a href="../#Examples-14">Examples</a></li></ul><li><a href="../#quotechar"><code>quotechar</code> / <code>openquotechar</code> / <code>closequotechar</code></a></li><ul><li><a href="../#Examples-15">Examples</a></li></ul><li><a href="../#escapechar"><code>escapechar</code></a></li><li><a href="../#dateformat"><code>dateformat</code></a></li><ul><li><a href="../#Examples-16">Examples</a></li></ul><li><a href="../#decimal"><code>decimal</code></a></li><ul><li><a href="../#Examples-17">Examples</a></li></ul><li><a href="../#truestrings"><code>truestrings</code> / <code>falsestrings</code></a></li><ul><li><a href="../#Examples-18">Examples</a></li></ul><li><a href="../#types"><code>types</code></a></li><ul><li><a href="../#Examples-19">Examples</a></li></ul><li><a href="../#typemap"><code>typemap</code></a></li><ul><li><a href="../#Examples-20">Examples</a></li></ul><li><a href="../#pool"><code>pool</code></a></li><ul><li><a href="../#Examples-21">Examples</a></li></ul><li><a href="../#downcast"><code>downcast</code></a></li><li><a href="../#stringtype"><code>stringtype</code></a></li><li><a href="../#strict"><code>strict</code> / <code>silencewarnings</code> / <code>maxwarnings</code></a></li><li><a href="../#debug"><code>debug</code></a></li></ul><li><a href="../#Common-terms">Common terms</a></li><ul><ul><li><a href="../#Standard-types">Standard types</a></li><li><a href="../#newlines">Newlines</a></li><li><a href="../#Cardinality">Cardinality</a></li></ul></ul><li><a href="../#Writing">Writing</a></li></ul><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CSV.read" href="#CSV.read"><code>CSV.read</code></a> — <span class="docstring-category">Function</span></header><section><div><p><code>CSV.read(source, sink::T; kwargs...)</code> =&gt; T</p><p>Read and parses a delimited file, materializing directly using the <code>sink</code> function.</p><p><strong>Arguments</strong></p><p><strong>File layout options:</strong></p><ul><li><code>header=1</code>: how column names should be determined; if given as an <code>Integer</code>, indicates the row to parse for column names; as an <code>AbstractVector{&lt;:Integer}</code>, indicates a set of rows to be concatenated together as column names; <code>Vector{Symbol}</code> or <code>Vector{String}</code> give column names explicitly (should match # of columns in dataset); if a dataset doesn&#39;t have column names, either provide them as a <code>Vector</code>, or set <code>header=0</code> or <code>header=false</code> and column names will be auto-generated (<code>Column1</code>, <code>Column2</code>, etc.). Note that if a row number header and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the header row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.</li><li><code>normalizenames::Bool=false</code>: whether column names should be &quot;normalized&quot; into valid Julia identifier symbols; useful when using the <code>tbl.col1</code> <code>getproperty</code> syntax or iterating rows and accessing column values of a row via <code>getproperty</code> (e.g. <code>row.col1</code>)</li><li><code>skipto::Integer</code>: specifies the row where the data starts in the csv file; by default, the next row after the <code>header</code> row(s) is used. If <code>header=0</code>, then the 1st row is assumed to be the start of data; providing a <code>skipto</code> argument does <em>not</em> affect the <code>header</code> argument. Note that if a row number <code>skipto</code> and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.</li><li><code>footerskip::Integer</code>: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the <code>comment</code> keyword argument) <em>do not</em> count towards the row number provided for <code>footerskip</code>, they are completely ignored by the parser</li><li><code>transpose::Bool</code>: read a csv file &quot;transposed&quot;, i.e. each column is parsed as a row</li><li><code>comment::String</code>: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or <code>skipto</code> and <code>comment</code> are provided, the header/data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.</li><li><code>ignoreemptyrows::Bool=true</code>: whether empty rows in a file should be ignored (if <code>false</code>, each column will be assigned <code>missing</code> for that empty row)</li><li><code>select</code>: an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;selector&quot; function of the form <code>(i, name) -&gt; keep::Bool</code>; only columns in the collection or for which the selector function returns <code>true</code> will be parsed and accessible in the resulting <code>CSV.File</code>. Invalid values in <code>select</code> are ignored.</li><li><code>drop</code>: inverse of <code>select</code>; an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;drop&quot; function of the form <code>(i, name) -&gt; drop::Bool</code>; columns in the collection or for which the drop function returns <code>true</code> will ignored in the resulting <code>CSV.File</code>. Invalid values in <code>drop</code> are ignored.</li><li><code>limit</code>: an <code>Integer</code> to indicate a limited number of rows to parse in a csv file; use in combination with <code>skipto</code> to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the <code>limit</code> argument may not result in an exact # of rows parsed; use <code>threaded=false</code> to ensure an exact limit if necessary</li><li><code>buffer_in_memory</code>: a <code>Bool</code>, default <code>false</code>, which controls whether a <code>Cmd</code>, <code>IO</code>, or gzipped source will be read/decompressed in memory vs. using a temporary file.</li><li><code>ntasks::Integer=Threads.nthreads()</code>: [not applicable to <code>CSV.Rows</code>] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the # of threads Julia was started with (i.e. <code>JULIA_NUM_THREADS</code> environment variable or <code>julia -t N</code>); setting <code>ntasks=1</code> will avoid any calls to <code>Threads.@spawn</code> and just read the file serially on the main thread; a single thread will also be used for smaller files by default (&lt; 5_000 cells)</li><li><code>rows_to_check::Integer=30</code>: [not applicable to <code>CSV.Rows</code>] a multithreaded parsed file will be split up into <code>ntasks</code> # of equal chunks; <code>rows_to_check</code> controls the # of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, <code>lines_to_check</code> may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows</li></ul><p><strong>Parsing options:</strong></p><ul><li><code>missingstring</code>: either a <code>nothing</code>, <code>String</code>, or <code>Vector{String}</code> to use as sentinel values that will be parsed as <code>missing</code>; if <code>nothing</code> is passed, no sentinel/missing values will be parsed; by default, <code>missingstring=&quot;&quot;</code>, which means only an empty field (two consecutive delimiters) is considered <code>missing</code></li><li><code>delim=&#39;,&#39;</code>: a <code>Char</code> or <code>String</code> that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file</li><li><code>ignorerepeated::Bool=false</code>: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells</li><li><code>quoted::Bool=true</code>: whether parsing should check for <code>quotechar</code> at the start/end of cells</li><li><code>quotechar=&#39;&quot;&#39;</code>, <code>openquotechar</code>, <code>closequotechar</code>: a <code>Char</code> (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters</li><li><code>escapechar=&#39;&quot;&#39;</code>: the <code>Char</code> used to escape quote characters in a quoted field</li><li><code>dateformat::Union{String, Dates.DateFormat, Nothing, AbstractDict}</code>: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an <code>AbstractDict</code>, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index <code>Int</code>, or name <code>Symbol</code> or <code>String</code> to the format string for that column.</li><li><code>decimal=&#39;.&#39;</code>: a <code>Char</code> indicating how decimals are separated in floats, i.e. <code>3.14</code> uses <code>&#39;.&#39;</code>, or <code>3,14</code> uses a comma <code>&#39;,&#39;</code></li><li><code>truestrings</code>, <code>falsestrings</code>: <code>Vector{String}</code>s that indicate how <code>true</code> or <code>false</code> values are represented; by default <code>&quot;true&quot;, &quot;True&quot;, &quot;TRUE&quot;, &quot;T&quot;, &quot;1&quot;</code> are used to detect <code>true</code> and <code>&quot;false&quot;, &quot;False&quot;, &quot;FALSE&quot;, &quot;F&quot;, &quot;0&quot;</code> are used to detect <code>false</code>; note that columns with only <code>1</code> and <code>0</code> values will default to <code>Int64</code> column type unless explicitly requested to be <code>Bool</code> via <code>types</code> keyword argument</li></ul><p><strong>Column Type Options:</strong></p><ul><li><code>types</code>: a single <code>Type</code>, <code>AbstractVector</code> or <code>AbstractDict</code> of types to be used for column types; if a single <code>Type</code> is provided, <em>all</em> columns will be parsed with that single type; an <code>AbstractDict</code> can map column index <code>Integer</code>, or name <code>Symbol</code> or <code>String</code> to type for a column, i.e. <code>Dict(1=&gt;Float64)</code> will set the first column as a <code>Float64</code>, <code>Dict(:column1=&gt;Float64)</code> will set the column named <code>column1</code> to <code>Float64</code> and, <code>Dict(&quot;column1&quot;=&gt;Float64)</code> will set the <code>column1</code> to <code>Float64</code>; if a <code>Vector</code> is provided, it must match the # of columns provided or detected in <code>header</code></li><li><code>typemap::Dict{Type, Type}</code>: a mapping of a type that should be replaced in every instance with another type, i.e. <code>Dict(Float64=&gt;String)</code> would change every detected <code>Float64</code> column to be parsed as <code>String</code>; only &quot;standard&quot; types are allowed to be mapped to another type, i.e. <code>Int64</code>, <code>Float64</code>, <code>Date</code>, <code>DateTime</code>, <code>Time</code>, and <code>Bool</code>. If a column of one of those types is &quot;detected&quot;, it will be mapped to the specified type.</li><li><code>pool::Union{Bool, Real, AbstractVector, AbstractDict}=0.25</code>: [not supported by <code>CSV.Rows</code>] controls whether columns will be built as <code>PooledArray</code>; if <code>true</code>, all columns detected as <code>String</code> will be pooled; alternatively, the proportion of unique values below which <code>String</code> columns should be pooled (by default 0.25, meaning that if the # of unique strings in a column is under 25.0%, it will be pooled); if an <code>AbstractVector</code>, each element should be <code>Bool</code> or <code>Real</code> and the # of elements should match the # of columns in the dataset; if an <code>AbstractDict</code>, a <code>Bool</code> or <code>Real</code> value can be provided for individual columns where the dict key is given as column index <code>Integer</code>, or column name as <code>Symbol</code> or <code>String</code></li><li><code>downcast::Bool=false</code>: controls whether columns detected as <code>Int64</code> will be &quot;downcast&quot; to the smallest possible integer type like <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, etc.</li><li><code>stringtype=WeakRefStrings.InlineString</code>: controls how detected string columns will ultimately be returned; default is <code>InlineString</code>, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to <code>String</code>. If <code>String</code> is passed, all string columns will just be normal <code>String</code> values. If <code>PosLenString</code> is passed, string columns will be returned as <code>PosLenStringVector</code>, which is a special &quot;lazy&quot; <code>AbstractVector</code> that acts as a &quot;view&quot; into the original file data. This can lead to the most efficient parsing times, but note that the &quot;view&quot; nature of <code>PosLenStringVector</code> makes it read-only, so operations like <code>push!</code>, <code>append!</code>, or <code>setindex!</code> are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail</li><li><code>strict::Bool=false</code>: whether invalid values should throw a parsing error or be replaced with <code>missing</code></li><li><code>silencewarnings::Bool=false</code>: if <code>strict=false</code>, whether invalid value warnings should be silenced</li><li><code>maxwarnings::Int=100</code>: if more than <code>maxwarnings</code> number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to <code>maxwarnings</code></li><li><code>debug::Bool=false</code>: passing <code>true</code> will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed</li></ul><p><strong>Iteration options:</strong></p><ul><li><code>reusebuffer=false</code>: [only supported by <code>CSV.Rows</code>] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it&#39;s not safe to use this option if doing <code>collect(CSV.Rows(file))</code> because only current iterated row is &quot;valid&quot;)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaData/CSV.jl/blob/e2a24c840a2b2cbd00a48c67f6c34c651de90dd4/src/CSV.jl#LL49-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CSV.File" href="#CSV.File"><code>CSV.File</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CSV.File(input; kwargs...) =&gt; CSV.File</code></pre><p>Read a UTF-8 CSV input and return a <code>CSV.File</code> object.</p><p>The <a href="../#input"><code>input</code></a> argument can be one of:</p><ul><li>filename given as a string or FilePaths.jl type</li><li>an <code>AbstractVector{UInt8}</code> byte buffer</li><li>a <code>Cmd</code> or other <code>IO</code></li><li>a csv-formatted string can be passed like <code>IOBuffer(str)</code></li><li>a gzipped file, which will automatically be decompressed for parsing</li></ul><p>To read a csv file from a url, use the Downloads.jl stdlib or HTTP.jl package, where the resulting downloaded tempfile or <code>HTTP.Response</code> body can be passed like:</p><pre><code class="language-julia hljs">using Downloads, CSV
f = CSV.File(Downloads.download(url))

# or

using HTTP, CSV
f = CSV.File(HTTP.get(url).body)</code></pre><p>Opens the file and uses passed arguments to detect the number of columns and column types, unless column types are provided manually via the <code>types</code> keyword argument. Note that passing column types manually can slightly increase performance for each column type provided (column types can be given as a <code>Vector</code> for all columns, or specified per column via name or index in a <code>Dict</code>).</p><p>For text encodings other than UTF-8, load the <a href="https://github.com/JuliaStrings/StringEncodings.jl">StringEncodings.jl</a> package and call e.g. <code>CSV.File(open(read, input, enc&quot;ISO-8859-1&quot;))</code>.</p><p>The returned <code>CSV.File</code> object supports the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface and can iterate <code>CSV.Row</code>s. <code>CSV.Row</code> supports <code>propertynames</code> and <code>getproperty</code> to access individual row values. <code>CSV.File</code> also supports entire column access like a <code>DataFrame</code> via direct property access on the file object, like <code>f = CSV.File(file); f.col1</code>. Note that duplicate column names will be detected and adjusted to ensure uniqueness (duplicate column name <code>a</code> will become <code>a_1</code>). For example, one could iterate over a csv file with column names <code>a</code>, <code>b</code>, and <code>c</code> by doing:</p><pre><code class="language-julia hljs">for row in CSV.File(file)
    println(&quot;a=$(row.a), b=$(row.b), c=$(row.c)&quot;)
end</code></pre><p>By supporting the Tables.jl interface, a <code>CSV.File</code> can also be a table input to any other table sink function. Like:</p><pre><code class="language-julia hljs"># materialize a csv file as a DataFrame, copying columns from CSV.File
df = CSV.File(file) |&gt; DataFrame

# to avoid making a copy of parsed columns, use CSV.read
df = CSV.read(file, DataFrame)

# load a csv file directly into an sqlite database table
db = SQLite.DB()
tbl = CSV.File(file) |&gt; SQLite.load!(db, &quot;sqlite_table&quot;)</code></pre><p><strong>Arguments</strong></p><p><strong>File layout options:</strong></p><ul><li><code>header=1</code>: how column names should be determined; if given as an <code>Integer</code>, indicates the row to parse for column names; as an <code>AbstractVector{&lt;:Integer}</code>, indicates a set of rows to be concatenated together as column names; <code>Vector{Symbol}</code> or <code>Vector{String}</code> give column names explicitly (should match # of columns in dataset); if a dataset doesn&#39;t have column names, either provide them as a <code>Vector</code>, or set <code>header=0</code> or <code>header=false</code> and column names will be auto-generated (<code>Column1</code>, <code>Column2</code>, etc.). Note that if a row number header and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the header row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.</li><li><code>normalizenames::Bool=false</code>: whether column names should be &quot;normalized&quot; into valid Julia identifier symbols; useful when using the <code>tbl.col1</code> <code>getproperty</code> syntax or iterating rows and accessing column values of a row via <code>getproperty</code> (e.g. <code>row.col1</code>)</li><li><code>skipto::Integer</code>: specifies the row where the data starts in the csv file; by default, the next row after the <code>header</code> row(s) is used. If <code>header=0</code>, then the 1st row is assumed to be the start of data; providing a <code>skipto</code> argument does <em>not</em> affect the <code>header</code> argument. Note that if a row number <code>skipto</code> and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.</li><li><code>footerskip::Integer</code>: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the <code>comment</code> keyword argument) <em>do not</em> count towards the row number provided for <code>footerskip</code>, they are completely ignored by the parser</li><li><code>transpose::Bool</code>: read a csv file &quot;transposed&quot;, i.e. each column is parsed as a row</li><li><code>comment::String</code>: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or <code>skipto</code> and <code>comment</code> are provided, the header/data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.</li><li><code>ignoreemptyrows::Bool=true</code>: whether empty rows in a file should be ignored (if <code>false</code>, each column will be assigned <code>missing</code> for that empty row)</li><li><code>select</code>: an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;selector&quot; function of the form <code>(i, name) -&gt; keep::Bool</code>; only columns in the collection or for which the selector function returns <code>true</code> will be parsed and accessible in the resulting <code>CSV.File</code>. Invalid values in <code>select</code> are ignored.</li><li><code>drop</code>: inverse of <code>select</code>; an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;drop&quot; function of the form <code>(i, name) -&gt; drop::Bool</code>; columns in the collection or for which the drop function returns <code>true</code> will ignored in the resulting <code>CSV.File</code>. Invalid values in <code>drop</code> are ignored.</li><li><code>limit</code>: an <code>Integer</code> to indicate a limited number of rows to parse in a csv file; use in combination with <code>skipto</code> to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the <code>limit</code> argument may not result in an exact # of rows parsed; use <code>threaded=false</code> to ensure an exact limit if necessary</li><li><code>buffer_in_memory</code>: a <code>Bool</code>, default <code>false</code>, which controls whether a <code>Cmd</code>, <code>IO</code>, or gzipped source will be read/decompressed in memory vs. using a temporary file.</li><li><code>ntasks::Integer=Threads.nthreads()</code>: [not applicable to <code>CSV.Rows</code>] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the # of threads Julia was started with (i.e. <code>JULIA_NUM_THREADS</code> environment variable or <code>julia -t N</code>); setting <code>ntasks=1</code> will avoid any calls to <code>Threads.@spawn</code> and just read the file serially on the main thread; a single thread will also be used for smaller files by default (&lt; 5_000 cells)</li><li><code>rows_to_check::Integer=30</code>: [not applicable to <code>CSV.Rows</code>] a multithreaded parsed file will be split up into <code>ntasks</code> # of equal chunks; <code>rows_to_check</code> controls the # of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, <code>lines_to_check</code> may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows</li></ul><p><strong>Parsing options:</strong></p><ul><li><code>missingstring</code>: either a <code>nothing</code>, <code>String</code>, or <code>Vector{String}</code> to use as sentinel values that will be parsed as <code>missing</code>; if <code>nothing</code> is passed, no sentinel/missing values will be parsed; by default, <code>missingstring=&quot;&quot;</code>, which means only an empty field (two consecutive delimiters) is considered <code>missing</code></li><li><code>delim=&#39;,&#39;</code>: a <code>Char</code> or <code>String</code> that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file</li><li><code>ignorerepeated::Bool=false</code>: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells</li><li><code>quoted::Bool=true</code>: whether parsing should check for <code>quotechar</code> at the start/end of cells</li><li><code>quotechar=&#39;&quot;&#39;</code>, <code>openquotechar</code>, <code>closequotechar</code>: a <code>Char</code> (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters</li><li><code>escapechar=&#39;&quot;&#39;</code>: the <code>Char</code> used to escape quote characters in a quoted field</li><li><code>dateformat::Union{String, Dates.DateFormat, Nothing, AbstractDict}</code>: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an <code>AbstractDict</code>, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index <code>Int</code>, or name <code>Symbol</code> or <code>String</code> to the format string for that column.</li><li><code>decimal=&#39;.&#39;</code>: a <code>Char</code> indicating how decimals are separated in floats, i.e. <code>3.14</code> uses <code>&#39;.&#39;</code>, or <code>3,14</code> uses a comma <code>&#39;,&#39;</code></li><li><code>truestrings</code>, <code>falsestrings</code>: <code>Vector{String}</code>s that indicate how <code>true</code> or <code>false</code> values are represented; by default <code>&quot;true&quot;, &quot;True&quot;, &quot;TRUE&quot;, &quot;T&quot;, &quot;1&quot;</code> are used to detect <code>true</code> and <code>&quot;false&quot;, &quot;False&quot;, &quot;FALSE&quot;, &quot;F&quot;, &quot;0&quot;</code> are used to detect <code>false</code>; note that columns with only <code>1</code> and <code>0</code> values will default to <code>Int64</code> column type unless explicitly requested to be <code>Bool</code> via <code>types</code> keyword argument</li></ul><p><strong>Column Type Options:</strong></p><ul><li><code>types</code>: a single <code>Type</code>, <code>AbstractVector</code> or <code>AbstractDict</code> of types to be used for column types; if a single <code>Type</code> is provided, <em>all</em> columns will be parsed with that single type; an <code>AbstractDict</code> can map column index <code>Integer</code>, or name <code>Symbol</code> or <code>String</code> to type for a column, i.e. <code>Dict(1=&gt;Float64)</code> will set the first column as a <code>Float64</code>, <code>Dict(:column1=&gt;Float64)</code> will set the column named <code>column1</code> to <code>Float64</code> and, <code>Dict(&quot;column1&quot;=&gt;Float64)</code> will set the <code>column1</code> to <code>Float64</code>; if a <code>Vector</code> is provided, it must match the # of columns provided or detected in <code>header</code></li><li><code>typemap::Dict{Type, Type}</code>: a mapping of a type that should be replaced in every instance with another type, i.e. <code>Dict(Float64=&gt;String)</code> would change every detected <code>Float64</code> column to be parsed as <code>String</code>; only &quot;standard&quot; types are allowed to be mapped to another type, i.e. <code>Int64</code>, <code>Float64</code>, <code>Date</code>, <code>DateTime</code>, <code>Time</code>, and <code>Bool</code>. If a column of one of those types is &quot;detected&quot;, it will be mapped to the specified type.</li><li><code>pool::Union{Bool, Real, AbstractVector, AbstractDict}=0.25</code>: [not supported by <code>CSV.Rows</code>] controls whether columns will be built as <code>PooledArray</code>; if <code>true</code>, all columns detected as <code>String</code> will be pooled; alternatively, the proportion of unique values below which <code>String</code> columns should be pooled (by default 0.25, meaning that if the # of unique strings in a column is under 25.0%, it will be pooled); if an <code>AbstractVector</code>, each element should be <code>Bool</code> or <code>Real</code> and the # of elements should match the # of columns in the dataset; if an <code>AbstractDict</code>, a <code>Bool</code> or <code>Real</code> value can be provided for individual columns where the dict key is given as column index <code>Integer</code>, or column name as <code>Symbol</code> or <code>String</code></li><li><code>downcast::Bool=false</code>: controls whether columns detected as <code>Int64</code> will be &quot;downcast&quot; to the smallest possible integer type like <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, etc.</li><li><code>stringtype=WeakRefStrings.InlineString</code>: controls how detected string columns will ultimately be returned; default is <code>InlineString</code>, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to <code>String</code>. If <code>String</code> is passed, all string columns will just be normal <code>String</code> values. If <code>PosLenString</code> is passed, string columns will be returned as <code>PosLenStringVector</code>, which is a special &quot;lazy&quot; <code>AbstractVector</code> that acts as a &quot;view&quot; into the original file data. This can lead to the most efficient parsing times, but note that the &quot;view&quot; nature of <code>PosLenStringVector</code> makes it read-only, so operations like <code>push!</code>, <code>append!</code>, or <code>setindex!</code> are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail</li><li><code>strict::Bool=false</code>: whether invalid values should throw a parsing error or be replaced with <code>missing</code></li><li><code>silencewarnings::Bool=false</code>: if <code>strict=false</code>, whether invalid value warnings should be silenced</li><li><code>maxwarnings::Int=100</code>: if more than <code>maxwarnings</code> number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to <code>maxwarnings</code></li><li><code>debug::Bool=false</code>: passing <code>true</code> will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed</li></ul><p><strong>Iteration options:</strong></p><ul><li><code>reusebuffer=false</code>: [only supported by <code>CSV.Rows</code>] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it&#39;s not safe to use this option if doing <code>collect(CSV.Rows(file))</code> because only current iterated row is &quot;valid&quot;)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaData/CSV.jl/blob/e2a24c840a2b2cbd00a48c67f6c34c651de90dd4/src/file.jl#LL92-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CSV.Chunks" href="#CSV.Chunks"><code>CSV.Chunks</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CSV.Chunks(source; ntasks::Integer=Threads.nthreads(), kwargs...) =&gt; CSV.Chunks</code></pre><p>Returns a file &quot;chunk&quot; iterator. Accepts all the same inputs and keyword arguments as <a href="#CSV.File"><code>CSV.File</code></a>, see those docs for explanations of each keyword argument.</p><p>The <code>ntasks</code> keyword argument specifies how many chunks a file should be split up into, defaulting to  the # of threads available to Julia (i.e. <code>JULIA_NUM_THREADS</code> environment variable) or 8 if Julia is run single-threaded.</p><p>Each iteration of <code>CSV.Chunks</code> produces the next chunk of a file as a <code>CSV.File</code>. While initial file metadata detection is done only once (to determine # of columns, column names, etc), each iteration does independent type inference on columns. This is significant as different chunks may end up with different column types than previous chunks as new values are encountered in the file. Note that, as with <code>CSV.File</code>, types may be passed manually via the <code>type</code> or <code>types</code> keyword arguments.</p><p>This functionality is new and thus considered experimental; please <a href="https://github.com/JuliaData/CSV.jl/issues/new">open an issue</a> if you run into any problems/bugs.</p><p><strong>Arguments</strong></p><p><strong>File layout options:</strong></p><ul><li><code>header=1</code>: how column names should be determined; if given as an <code>Integer</code>, indicates the row to parse for column names; as an <code>AbstractVector{&lt;:Integer}</code>, indicates a set of rows to be concatenated together as column names; <code>Vector{Symbol}</code> or <code>Vector{String}</code> give column names explicitly (should match # of columns in dataset); if a dataset doesn&#39;t have column names, either provide them as a <code>Vector</code>, or set <code>header=0</code> or <code>header=false</code> and column names will be auto-generated (<code>Column1</code>, <code>Column2</code>, etc.). Note that if a row number header and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the header row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.</li><li><code>normalizenames::Bool=false</code>: whether column names should be &quot;normalized&quot; into valid Julia identifier symbols; useful when using the <code>tbl.col1</code> <code>getproperty</code> syntax or iterating rows and accessing column values of a row via <code>getproperty</code> (e.g. <code>row.col1</code>)</li><li><code>skipto::Integer</code>: specifies the row where the data starts in the csv file; by default, the next row after the <code>header</code> row(s) is used. If <code>header=0</code>, then the 1st row is assumed to be the start of data; providing a <code>skipto</code> argument does <em>not</em> affect the <code>header</code> argument. Note that if a row number <code>skipto</code> and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.</li><li><code>footerskip::Integer</code>: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the <code>comment</code> keyword argument) <em>do not</em> count towards the row number provided for <code>footerskip</code>, they are completely ignored by the parser</li><li><code>transpose::Bool</code>: read a csv file &quot;transposed&quot;, i.e. each column is parsed as a row</li><li><code>comment::String</code>: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or <code>skipto</code> and <code>comment</code> are provided, the header/data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.</li><li><code>ignoreemptyrows::Bool=true</code>: whether empty rows in a file should be ignored (if <code>false</code>, each column will be assigned <code>missing</code> for that empty row)</li><li><code>select</code>: an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;selector&quot; function of the form <code>(i, name) -&gt; keep::Bool</code>; only columns in the collection or for which the selector function returns <code>true</code> will be parsed and accessible in the resulting <code>CSV.File</code>. Invalid values in <code>select</code> are ignored.</li><li><code>drop</code>: inverse of <code>select</code>; an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;drop&quot; function of the form <code>(i, name) -&gt; drop::Bool</code>; columns in the collection or for which the drop function returns <code>true</code> will ignored in the resulting <code>CSV.File</code>. Invalid values in <code>drop</code> are ignored.</li><li><code>limit</code>: an <code>Integer</code> to indicate a limited number of rows to parse in a csv file; use in combination with <code>skipto</code> to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the <code>limit</code> argument may not result in an exact # of rows parsed; use <code>threaded=false</code> to ensure an exact limit if necessary</li><li><code>buffer_in_memory</code>: a <code>Bool</code>, default <code>false</code>, which controls whether a <code>Cmd</code>, <code>IO</code>, or gzipped source will be read/decompressed in memory vs. using a temporary file.</li><li><code>ntasks::Integer=Threads.nthreads()</code>: [not applicable to <code>CSV.Rows</code>] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the # of threads Julia was started with (i.e. <code>JULIA_NUM_THREADS</code> environment variable or <code>julia -t N</code>); setting <code>ntasks=1</code> will avoid any calls to <code>Threads.@spawn</code> and just read the file serially on the main thread; a single thread will also be used for smaller files by default (&lt; 5_000 cells)</li><li><code>rows_to_check::Integer=30</code>: [not applicable to <code>CSV.Rows</code>] a multithreaded parsed file will be split up into <code>ntasks</code> # of equal chunks; <code>rows_to_check</code> controls the # of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, <code>lines_to_check</code> may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows</li></ul><p><strong>Parsing options:</strong></p><ul><li><code>missingstring</code>: either a <code>nothing</code>, <code>String</code>, or <code>Vector{String}</code> to use as sentinel values that will be parsed as <code>missing</code>; if <code>nothing</code> is passed, no sentinel/missing values will be parsed; by default, <code>missingstring=&quot;&quot;</code>, which means only an empty field (two consecutive delimiters) is considered <code>missing</code></li><li><code>delim=&#39;,&#39;</code>: a <code>Char</code> or <code>String</code> that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file</li><li><code>ignorerepeated::Bool=false</code>: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells</li><li><code>quoted::Bool=true</code>: whether parsing should check for <code>quotechar</code> at the start/end of cells</li><li><code>quotechar=&#39;&quot;&#39;</code>, <code>openquotechar</code>, <code>closequotechar</code>: a <code>Char</code> (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters</li><li><code>escapechar=&#39;&quot;&#39;</code>: the <code>Char</code> used to escape quote characters in a quoted field</li><li><code>dateformat::Union{String, Dates.DateFormat, Nothing, AbstractDict}</code>: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an <code>AbstractDict</code>, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index <code>Int</code>, or name <code>Symbol</code> or <code>String</code> to the format string for that column.</li><li><code>decimal=&#39;.&#39;</code>: a <code>Char</code> indicating how decimals are separated in floats, i.e. <code>3.14</code> uses <code>&#39;.&#39;</code>, or <code>3,14</code> uses a comma <code>&#39;,&#39;</code></li><li><code>truestrings</code>, <code>falsestrings</code>: <code>Vector{String}</code>s that indicate how <code>true</code> or <code>false</code> values are represented; by default <code>&quot;true&quot;, &quot;True&quot;, &quot;TRUE&quot;, &quot;T&quot;, &quot;1&quot;</code> are used to detect <code>true</code> and <code>&quot;false&quot;, &quot;False&quot;, &quot;FALSE&quot;, &quot;F&quot;, &quot;0&quot;</code> are used to detect <code>false</code>; note that columns with only <code>1</code> and <code>0</code> values will default to <code>Int64</code> column type unless explicitly requested to be <code>Bool</code> via <code>types</code> keyword argument</li></ul><p><strong>Column Type Options:</strong></p><ul><li><code>types</code>: a single <code>Type</code>, <code>AbstractVector</code> or <code>AbstractDict</code> of types to be used for column types; if a single <code>Type</code> is provided, <em>all</em> columns will be parsed with that single type; an <code>AbstractDict</code> can map column index <code>Integer</code>, or name <code>Symbol</code> or <code>String</code> to type for a column, i.e. <code>Dict(1=&gt;Float64)</code> will set the first column as a <code>Float64</code>, <code>Dict(:column1=&gt;Float64)</code> will set the column named <code>column1</code> to <code>Float64</code> and, <code>Dict(&quot;column1&quot;=&gt;Float64)</code> will set the <code>column1</code> to <code>Float64</code>; if a <code>Vector</code> is provided, it must match the # of columns provided or detected in <code>header</code></li><li><code>typemap::Dict{Type, Type}</code>: a mapping of a type that should be replaced in every instance with another type, i.e. <code>Dict(Float64=&gt;String)</code> would change every detected <code>Float64</code> column to be parsed as <code>String</code>; only &quot;standard&quot; types are allowed to be mapped to another type, i.e. <code>Int64</code>, <code>Float64</code>, <code>Date</code>, <code>DateTime</code>, <code>Time</code>, and <code>Bool</code>. If a column of one of those types is &quot;detected&quot;, it will be mapped to the specified type.</li><li><code>pool::Union{Bool, Real, AbstractVector, AbstractDict}=0.25</code>: [not supported by <code>CSV.Rows</code>] controls whether columns will be built as <code>PooledArray</code>; if <code>true</code>, all columns detected as <code>String</code> will be pooled; alternatively, the proportion of unique values below which <code>String</code> columns should be pooled (by default 0.25, meaning that if the # of unique strings in a column is under 25.0%, it will be pooled); if an <code>AbstractVector</code>, each element should be <code>Bool</code> or <code>Real</code> and the # of elements should match the # of columns in the dataset; if an <code>AbstractDict</code>, a <code>Bool</code> or <code>Real</code> value can be provided for individual columns where the dict key is given as column index <code>Integer</code>, or column name as <code>Symbol</code> or <code>String</code></li><li><code>downcast::Bool=false</code>: controls whether columns detected as <code>Int64</code> will be &quot;downcast&quot; to the smallest possible integer type like <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, etc.</li><li><code>stringtype=WeakRefStrings.InlineString</code>: controls how detected string columns will ultimately be returned; default is <code>InlineString</code>, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to <code>String</code>. If <code>String</code> is passed, all string columns will just be normal <code>String</code> values. If <code>PosLenString</code> is passed, string columns will be returned as <code>PosLenStringVector</code>, which is a special &quot;lazy&quot; <code>AbstractVector</code> that acts as a &quot;view&quot; into the original file data. This can lead to the most efficient parsing times, but note that the &quot;view&quot; nature of <code>PosLenStringVector</code> makes it read-only, so operations like <code>push!</code>, <code>append!</code>, or <code>setindex!</code> are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail</li><li><code>strict::Bool=false</code>: whether invalid values should throw a parsing error or be replaced with <code>missing</code></li><li><code>silencewarnings::Bool=false</code>: if <code>strict=false</code>, whether invalid value warnings should be silenced</li><li><code>maxwarnings::Int=100</code>: if more than <code>maxwarnings</code> number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to <code>maxwarnings</code></li><li><code>debug::Bool=false</code>: passing <code>true</code> will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed</li></ul><p><strong>Iteration options:</strong></p><ul><li><code>reusebuffer=false</code>: [only supported by <code>CSV.Rows</code>] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it&#39;s not safe to use this option if doing <code>collect(CSV.Rows(file))</code> because only current iterated row is &quot;valid&quot;)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaData/CSV.jl/blob/e2a24c840a2b2cbd00a48c67f6c34c651de90dd4/src/chunks.jl#LL6-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CSV.Rows" href="#CSV.Rows"><code>CSV.Rows</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CSV.Rows(source; kwargs...) =&gt; CSV.Rows</code></pre><p>Read a csv input returning a <code>CSV.Rows</code> object.</p><p>The <code>source</code> argument can be one of:</p><ul><li>filename given as a string or FilePaths.jl type</li><li>an <code>AbstractVector{UInt8}</code> like a byte buffer or <code>codeunits(string)</code></li><li>an <code>IOBuffer</code></li></ul><p>To read a csv file from a url, use the HTTP.jl package, where the <code>HTTP.Response</code> body can be passed like:</p><pre><code class="language-julia hljs">f = CSV.Rows(HTTP.get(url).body)</code></pre><p>For other <code>IO</code> or <code>Cmd</code> inputs, you can pass them like: <code>f = CSV.Rows(read(obj))</code>.</p><p>While similar to <a href="#CSV.File"><code>CSV.File</code></a>, <code>CSV.Rows</code> provides a slightly different interface, the tradeoffs including:</p><ul><li>Very minimal memory footprint; while iterating, only the current row values are buffered</li><li>Only provides row access via iteration; to access columns, one can stream the rows into a table type</li><li>Performs no type inference; each column/cell is essentially treated as <code>Union{String, Missing}</code>, users can utilize the performant <code>Parsers.parse(T, str)</code> to convert values to a more specific type if needed, or pass types upon construction using the <code>type</code> or <code>types</code> keyword arguments</li></ul><p>Opens the file and uses passed arguments to detect the number of columns, ***but not*** column types (column types default to <code>String</code> unless otherwise manually provided). The returned <code>CSV.Rows</code> object supports the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface and can iterate rows. Each row object supports <code>propertynames</code>, <code>getproperty</code>, and <code>getindex</code> to access individual row values. Note that duplicate column names will be detected and adjusted to ensure uniqueness (duplicate column name <code>a</code> will become <code>a_1</code>). For example, one could iterate over a csv file with column names <code>a</code>, <code>b</code>, and <code>c</code> by doing:</p><pre><code class="language-julia hljs">for row in CSV.Rows(file)
    println(&quot;a=$(row.a), b=$(row.b), c=$(row.c)&quot;)
end</code></pre><p><strong>Arguments</strong></p><p><strong>File layout options:</strong></p><ul><li><code>header=1</code>: how column names should be determined; if given as an <code>Integer</code>, indicates the row to parse for column names; as an <code>AbstractVector{&lt;:Integer}</code>, indicates a set of rows to be concatenated together as column names; <code>Vector{Symbol}</code> or <code>Vector{String}</code> give column names explicitly (should match # of columns in dataset); if a dataset doesn&#39;t have column names, either provide them as a <code>Vector</code>, or set <code>header=0</code> or <code>header=false</code> and column names will be auto-generated (<code>Column1</code>, <code>Column2</code>, etc.). Note that if a row number header and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the header row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header row will actually be the next non-commented row.</li><li><code>normalizenames::Bool=false</code>: whether column names should be &quot;normalized&quot; into valid Julia identifier symbols; useful when using the <code>tbl.col1</code> <code>getproperty</code> syntax or iterating rows and accessing column values of a row via <code>getproperty</code> (e.g. <code>row.col1</code>)</li><li><code>skipto::Integer</code>: specifies the row where the data starts in the csv file; by default, the next row after the <code>header</code> row(s) is used. If <code>header=0</code>, then the 1st row is assumed to be the start of data; providing a <code>skipto</code> argument does <em>not</em> affect the <code>header</code> argument. Note that if a row number <code>skipto</code> and <code>comment</code> or <code>ignoreemptyrows</code> are provided, the data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the data row will actually be the next non-commented row.</li><li><code>footerskip::Integer</code>: number of rows at the end of a file to skip parsing.  Do note that commented rows (see the <code>comment</code> keyword argument) <em>do not</em> count towards the row number provided for <code>footerskip</code>, they are completely ignored by the parser</li><li><code>transpose::Bool</code>: read a csv file &quot;transposed&quot;, i.e. each column is parsed as a row</li><li><code>comment::String</code>: string that will cause rows that begin with it to be skipped while parsing. Note that if a row number header or <code>skipto</code> and <code>comment</code> are provided, the header/data row will be the first non-commented/non-empty row <em>after</em> the row number, meaning if the provided row number is a commented row, the header/data row will actually be the next non-commented row.</li><li><code>ignoreemptyrows::Bool=true</code>: whether empty rows in a file should be ignored (if <code>false</code>, each column will be assigned <code>missing</code> for that empty row)</li><li><code>select</code>: an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;selector&quot; function of the form <code>(i, name) -&gt; keep::Bool</code>; only columns in the collection or for which the selector function returns <code>true</code> will be parsed and accessible in the resulting <code>CSV.File</code>. Invalid values in <code>select</code> are ignored.</li><li><code>drop</code>: inverse of <code>select</code>; an <code>AbstractVector</code> of <code>Integer</code>, <code>Symbol</code>, <code>String</code>, or <code>Bool</code>, or a &quot;drop&quot; function of the form <code>(i, name) -&gt; drop::Bool</code>; columns in the collection or for which the drop function returns <code>true</code> will ignored in the resulting <code>CSV.File</code>. Invalid values in <code>drop</code> are ignored.</li><li><code>limit</code>: an <code>Integer</code> to indicate a limited number of rows to parse in a csv file; use in combination with <code>skipto</code> to read a specific, contiguous chunk within a file; note for large files when multiple threads are used for parsing, the <code>limit</code> argument may not result in an exact # of rows parsed; use <code>threaded=false</code> to ensure an exact limit if necessary</li><li><code>buffer_in_memory</code>: a <code>Bool</code>, default <code>false</code>, which controls whether a <code>Cmd</code>, <code>IO</code>, or gzipped source will be read/decompressed in memory vs. using a temporary file.</li><li><code>ntasks::Integer=Threads.nthreads()</code>: [not applicable to <code>CSV.Rows</code>] for multithreaded parsed files, this controls the number of tasks spawned to read a file in concurrent chunks; defaults to the # of threads Julia was started with (i.e. <code>JULIA_NUM_THREADS</code> environment variable or <code>julia -t N</code>); setting <code>ntasks=1</code> will avoid any calls to <code>Threads.@spawn</code> and just read the file serially on the main thread; a single thread will also be used for smaller files by default (&lt; 5_000 cells)</li><li><code>rows_to_check::Integer=30</code>: [not applicable to <code>CSV.Rows</code>] a multithreaded parsed file will be split up into <code>ntasks</code> # of equal chunks; <code>rows_to_check</code> controls the # of rows are checked to ensure parsing correctly found valid rows; for certain files with very large quoted text fields, <code>lines_to_check</code> may need to be higher (10, 30, etc.) to ensure parsing correctly finds these rows</li></ul><p><strong>Parsing options:</strong></p><ul><li><code>missingstring</code>: either a <code>nothing</code>, <code>String</code>, or <code>Vector{String}</code> to use as sentinel values that will be parsed as <code>missing</code>; if <code>nothing</code> is passed, no sentinel/missing values will be parsed; by default, <code>missingstring=&quot;&quot;</code>, which means only an empty field (two consecutive delimiters) is considered <code>missing</code></li><li><code>delim=&#39;,&#39;</code>: a <code>Char</code> or <code>String</code> that indicates how columns are delimited in a file; if no argument is provided, parsing will try to detect the most consistent delimiter on the first 10 rows of the file</li><li><code>ignorerepeated::Bool=false</code>: whether repeated (consecutive/sequential) delimiters should be ignored while parsing; useful for fixed-width files with delimiter padding between cells</li><li><code>quoted::Bool=true</code>: whether parsing should check for <code>quotechar</code> at the start/end of cells</li><li><code>quotechar=&#39;&quot;&#39;</code>, <code>openquotechar</code>, <code>closequotechar</code>: a <code>Char</code> (or different start and end characters) that indicate a quoted field which may contain textual delimiters or newline characters</li><li><code>escapechar=&#39;&quot;&#39;</code>: the <code>Char</code> used to escape quote characters in a quoted field</li><li><code>dateformat::Union{String, Dates.DateFormat, Nothing, AbstractDict}</code>: a date format string to indicate how Date/DateTime columns are formatted for the entire file; if given as an <code>AbstractDict</code>, date format strings to indicate how the Date/DateTime columns corresponding to the keys are formatted. The Dict can map column index <code>Int</code>, or name <code>Symbol</code> or <code>String</code> to the format string for that column.</li><li><code>decimal=&#39;.&#39;</code>: a <code>Char</code> indicating how decimals are separated in floats, i.e. <code>3.14</code> uses <code>&#39;.&#39;</code>, or <code>3,14</code> uses a comma <code>&#39;,&#39;</code></li><li><code>truestrings</code>, <code>falsestrings</code>: <code>Vector{String}</code>s that indicate how <code>true</code> or <code>false</code> values are represented; by default <code>&quot;true&quot;, &quot;True&quot;, &quot;TRUE&quot;, &quot;T&quot;, &quot;1&quot;</code> are used to detect <code>true</code> and <code>&quot;false&quot;, &quot;False&quot;, &quot;FALSE&quot;, &quot;F&quot;, &quot;0&quot;</code> are used to detect <code>false</code>; note that columns with only <code>1</code> and <code>0</code> values will default to <code>Int64</code> column type unless explicitly requested to be <code>Bool</code> via <code>types</code> keyword argument</li></ul><p><strong>Column Type Options:</strong></p><ul><li><code>types</code>: a single <code>Type</code>, <code>AbstractVector</code> or <code>AbstractDict</code> of types to be used for column types; if a single <code>Type</code> is provided, <em>all</em> columns will be parsed with that single type; an <code>AbstractDict</code> can map column index <code>Integer</code>, or name <code>Symbol</code> or <code>String</code> to type for a column, i.e. <code>Dict(1=&gt;Float64)</code> will set the first column as a <code>Float64</code>, <code>Dict(:column1=&gt;Float64)</code> will set the column named <code>column1</code> to <code>Float64</code> and, <code>Dict(&quot;column1&quot;=&gt;Float64)</code> will set the <code>column1</code> to <code>Float64</code>; if a <code>Vector</code> is provided, it must match the # of columns provided or detected in <code>header</code></li><li><code>typemap::Dict{Type, Type}</code>: a mapping of a type that should be replaced in every instance with another type, i.e. <code>Dict(Float64=&gt;String)</code> would change every detected <code>Float64</code> column to be parsed as <code>String</code>; only &quot;standard&quot; types are allowed to be mapped to another type, i.e. <code>Int64</code>, <code>Float64</code>, <code>Date</code>, <code>DateTime</code>, <code>Time</code>, and <code>Bool</code>. If a column of one of those types is &quot;detected&quot;, it will be mapped to the specified type.</li><li><code>pool::Union{Bool, Real, AbstractVector, AbstractDict}=0.25</code>: [not supported by <code>CSV.Rows</code>] controls whether columns will be built as <code>PooledArray</code>; if <code>true</code>, all columns detected as <code>String</code> will be pooled; alternatively, the proportion of unique values below which <code>String</code> columns should be pooled (by default 0.25, meaning that if the # of unique strings in a column is under 25.0%, it will be pooled); if an <code>AbstractVector</code>, each element should be <code>Bool</code> or <code>Real</code> and the # of elements should match the # of columns in the dataset; if an <code>AbstractDict</code>, a <code>Bool</code> or <code>Real</code> value can be provided for individual columns where the dict key is given as column index <code>Integer</code>, or column name as <code>Symbol</code> or <code>String</code></li><li><code>downcast::Bool=false</code>: controls whether columns detected as <code>Int64</code> will be &quot;downcast&quot; to the smallest possible integer type like <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, etc.</li><li><code>stringtype=WeakRefStrings.InlineString</code>: controls how detected string columns will ultimately be returned; default is <code>InlineString</code>, which stores string data in a fixed-size primitive type that helps avoid excessive heap memory usage; if a column has values longer than 32 bytes, it will default to <code>String</code>. If <code>String</code> is passed, all string columns will just be normal <code>String</code> values. If <code>PosLenString</code> is passed, string columns will be returned as <code>PosLenStringVector</code>, which is a special &quot;lazy&quot; <code>AbstractVector</code> that acts as a &quot;view&quot; into the original file data. This can lead to the most efficient parsing times, but note that the &quot;view&quot; nature of <code>PosLenStringVector</code> makes it read-only, so operations like <code>push!</code>, <code>append!</code>, or <code>setindex!</code> are not supported. It also keeps a reference to the entire input dataset source, so trying to modify or delete the underlying file, for example, may fail</li><li><code>strict::Bool=false</code>: whether invalid values should throw a parsing error or be replaced with <code>missing</code></li><li><code>silencewarnings::Bool=false</code>: if <code>strict=false</code>, whether invalid value warnings should be silenced</li><li><code>maxwarnings::Int=100</code>: if more than <code>maxwarnings</code> number of warnings are printed while parsing, further warnings will be silenced by default; for multithreaded parsing, each parsing task will print up to <code>maxwarnings</code></li><li><code>debug::Bool=false</code>: passing <code>true</code> will result in many informational prints while a dataset is parsed; can be useful when reporting issues or figuring out what is going on internally while a dataset is parsed</li></ul><p><strong>Iteration options:</strong></p><ul><li><code>reusebuffer=false</code>: [only supported by <code>CSV.Rows</code>] while iterating, whether a single row buffer should be allocated and reused on each iteration; only use if each row will be iterated once and not re-used (e.g. it&#39;s not safe to use this option if doing <code>collect(CSV.Rows(file))</code> because only current iterated row is &quot;valid&quot;)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaData/CSV.jl/blob/e2a24c840a2b2cbd00a48c67f6c34c651de90dd4/src/rows.jl#LL30-L65">source</a></section></article><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CSV.detect" href="#CSV.detect"><code>CSV.detect</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">CSV.detect(str::String)</code></pre><p>Use the same logic used by <code>CSV.File</code> to detect column types, to parse a value from a plain string. This can be useful in conjunction with the <code>CSV.Rows</code> type, which returns each cell of a file as a String. The order of types attempted is: <code>Int64</code>, <code>Float64</code>, <code>Date</code>, <code>DateTime</code>, <code>Bool</code>, and if all fail, the input String is returned. No errors are thrown. For advanced usage, you can pass your own <code>Parsers.Options</code> type as a keyword argument <code>option=ops</code> for sentinel value detection.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaData/CSV.jl/blob/e2a24c840a2b2cbd00a48c67f6c34c651de90dd4/src/utils.jl#LL284-L292">source</a></section></article><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="stringencodings"><a class="docs-heading-anchor" href="#stringencodings">Non-UTF-8 character encodings</a><a id="stringencodings-1"></a><a class="docs-heading-anchor-permalink" href="#stringencodings" title="Permalink"></a></h3><pre><code class="language-julia hljs"># assume I have csv text data encoded in ISO-8859-1 encoding
# I load the StringEncodings package, which provides encoding conversion functionality
using CSV, StringEncodings

# I open my `iso8859_encoded_file.csv` with the `enc&quot;ISO-8859-1&quot;` encoding
# and pass the opened IO object to `CSV.File`, which will read the entire
# input into a temporary file, then parse the data from the temp file
file = CSV.File(open(&quot;iso8859_encoded_file.csv&quot;, enc&quot;ISO-8859-1&quot;))

# to instead have the encoding conversion happen in memory, pass
# `buffer_in_memory=true`; this can be faster, but obviously results
# in more memory being used rather than disk via a temp file
file = CSV.File(open(&quot;iso8859_encoded_file.csv&quot;, enc&quot;ISO-8859-1&quot;); buffer_in_memory=true)</code></pre><h3 id="gzipped_input"><a class="docs-heading-anchor" href="#gzipped_input">Gzipped input</a><a id="gzipped_input-1"></a><a class="docs-heading-anchor-permalink" href="#gzipped_input" title="Permalink"></a></h3><pre><code class="language-julia hljs"># assume I have csv text data compressed via gzip
# no additional packages are needed; CSV.jl can decompress automatically
using CSV

# pass name of gzipped input file directly; data will be decompressed to a
# temporary file, then mmapped as a byte buffer for actual parsing
file = CSV.File(&quot;data.gz&quot;)

# to instead have the decompression happen in memory, pass
# `buffer_in_memory=true`; this can be faster, but obviously results
# in more memory being used rather than disk via a temp file
file = CSV.File(&quot;data.gz&quot;; buffer_in_memory=true)</code></pre><h3 id="csv_string"><a class="docs-heading-anchor" href="#csv_string">Delimited data in a string</a><a id="csv_string-1"></a><a class="docs-heading-anchor-permalink" href="#csv_string" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# I have csv data in a string I want to parse
data = &quot;&quot;&quot;
a,b,c
1,2,3
4,5,6
&quot;&quot;&quot;

# Calling `IOBuffer` on a string returns an in-memory IO object
# of the string data, which can be passed to `CSV.File` for parsing
file = CSV.File(IOBuffer(data))</code></pre><h3 id="http"><a class="docs-heading-anchor" href="#http">Data from the web/a url</a><a id="http-1"></a><a class="docs-heading-anchor-permalink" href="#http" title="Permalink"></a></h3><pre><code class="language-julia hljs"># assume there&#39;s delimited data I want to read from the web
# one option is to use the HTTP.jl package
using CSV, HTTP

# I first make the web request to get the data via `HTTP.get` on the `url`
http_response = HTTP.get(url)

# I can then access the data of the response as a `Vector{UInt8}` and pass
# it directly to `CSV.File` for parsing
file = CSV.File(http_response.body)

# another option, with Julia 1.6+, is using the Downloads stdlib
using Downloads
http_response = Downloads.download(url)

# by default, `Downloads.download` writes the response data to a temporary file
# which can then be passed to `CSV.File` for parsing
file = CSV.File(http_response)</code></pre><h4 id="zip_example"><a class="docs-heading-anchor" href="#zip_example">Reading from a zip file</a><a id="zip_example-1"></a><a class="docs-heading-anchor-permalink" href="#zip_example" title="Permalink"></a></h4><pre><code class="language-julia hljs">using ZipFile, CSV, DataFrames

a = DataFrame(a = 1:3)
CSV.write(&quot;a.csv&quot;, a)

# zip the file; Windows users who do not have zip available on the PATH can manually zip the CSV
# or write directly into the zip archive as shown below
;zip a.zip a.csv

# alternatively, write directly into the zip archive (without creating an unzipped csv file first)
z = ZipFile.Writer(&quot;a2.zip&quot;)
f = ZipFile.addfile(z, &quot;a.csv&quot;, method=ZipFile.Deflate)
a |&gt; CSV.write(f)
close(z)

# read file from zip archive
z = ZipFile.Reader(&quot;a.zip&quot;) # or &quot;a2.zip&quot;

# identify the right file in zip
a_file_in_zip = filter(x-&gt;x.name == &quot;a.csv&quot;, z.files)[1]

a_copy = CSV.File(a_file_in_zip) |&gt; DataFrame

a == a_copy

close(z)</code></pre><h3 id="second_row_header"><a class="docs-heading-anchor" href="#second_row_header">Column names on 2nd row</a><a id="second_row_header-1"></a><a class="docs-heading-anchor-permalink" href="#second_row_header" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

data = &quot;&quot;&quot;
descriptive row with information about the file that we&#39;d like to ignore
a,b,c
1,2,3
4,5,6
&quot;&quot;&quot;

# by passing header=2, parsing will ignore the 1st row entirely
# then parse the column names on row 2, then by default, it assumes
# the data starts on the row after the column names (row 3 in this case)
# which is correct for this case
file = CSV.File(IOBuffer(data); header=2)</code></pre><h3 id="no_header"><a class="docs-heading-anchor" href="#no_header">No column names in data</a><a id="no_header-1"></a><a class="docs-heading-anchor-permalink" href="#no_header" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# in this case, our data doesn&#39;t have any column names
data = &quot;&quot;&quot;
1,2,3
4,5,6
&quot;&quot;&quot;

# by passing `header=false`, parsing won&#39;t worry about looking for column names
# anywhere, but instead just start parsing the data and generate column names
# as needed, like `Column1`, `Column2`, and `Column3` in this case
file = CSV.File(IOBuffer(data); header=false)</code></pre><h3 id="manual_header"><a class="docs-heading-anchor" href="#manual_header">Manually provide column names</a><a id="manual_header-1"></a><a class="docs-heading-anchor-permalink" href="#manual_header" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# in this case, our data doesn&#39;t have any column names
data = &quot;&quot;&quot;
1,2,3
4,5,6
&quot;&quot;&quot;

# instead of passing `header=false` and getting auto-generated column names,
# we can instead pass the column names ourselves
file = CSV.File(IOBuffer(data); header=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])

# we can also pass the column names as Symbols; a copy of the manually provided
# column names will always be made and then converted to `Vector{Symbol}`
file = CSV.File(IOBuffer(data); header=[:a, :b, :c])</code></pre><h3 id="multi_row_header"><a class="docs-heading-anchor" href="#multi_row_header">Multi-row column names</a><a id="multi_row_header-1"></a><a class="docs-heading-anchor-permalink" href="#multi_row_header" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# in this case, our column names are `col_a`, `col_b`, and `col_c`,
# but split over the first and second rows
data = &quot;&quot;&quot;
col,col,col
a,b,c
1,2,3
4,5,6
&quot;&quot;&quot;

# by passing a collection of integers, parsing will parse each row in the collection
# and concatenate the values for each column, separating rows with `_` character
file = CSV.File(IOBuffer(data); header=[1, 2])</code></pre><h3 id="normalize_header"><a class="docs-heading-anchor" href="#normalize_header">Normalizing column names</a><a id="normalize_header-1"></a><a class="docs-heading-anchor-permalink" href="#normalize_header" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# in this case, our data are single letters, with column names of &quot;1&quot;, &quot;2&quot;, and &quot;3&quot;
# A single digit isn&#39;t a valid identifier in Julia, meaning we couldn&#39;t do something
# like `1 = 2 + 2`, where `1` would be a variable name
data = &quot;&quot;&quot;
1,2,3
a,b,c
d,e,f
h,i,j
&quot;&quot;&quot;

# in order to have valid identifiers for column names, we can pass
# `normalizenames=true`, which result in our column names becoming &quot;_1&quot;, &quot;_2&quot;, and &quot;_3&quot;
# note this isn&#39;t required, but can be convenient in certain cases
file = CSV.File(IOBuffer(data); normalizenames=true)

# we can acces the first column like
file._1

# another example where we may want to normalize is column names with spaces in them
data = &quot;&quot;&quot;
column one,column two, column three
1,2,3
4,5,6
&quot;&quot;&quot;

# normalizing will result in column names like &quot;column_one&quot;, &quot;column_two&quot; and &quot;column_three&quot;
file = CSV.File(IOBuffer(data); normalizenames=true)</code></pre><h3 id="skipto_example"><a class="docs-heading-anchor" href="#skipto_example">Skip to specific row where data starts</a><a id="skipto_example-1"></a><a class="docs-heading-anchor-permalink" href="#skipto_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# our data has a first row that we&#39;d like to ignore; our data also doesn&#39;t have
# column names, so we&#39;d like them to be auto-generated
data = &quot;&quot;&quot;
descriptive row that gives information about the data that we&#39;d like to ignore
1,2,3
4,5,6
&quot;&quot;&quot;

# with no column names in the data, we first pass `header=false`; by itself,
# this would result in parsing starting on row 1 to parse the actual data;
# but we&#39;d like to ignore the first row, so we pass `skipto=2` to skip over
# the first row; our colum names will be generated like `Column1`, `Column2`, `Column3`
file = CSV.File(IOBuffer(data); header=false, skipto=2)</code></pre><h3 id="footerskip_example"><a class="docs-heading-anchor" href="#footerskip_example">Skipping trailing useless rows</a><a id="footerskip_example-1"></a><a class="docs-heading-anchor-permalink" href="#footerskip_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# our data has column names of &quot;a&quot;, &quot;b&quot;, and &quot;c&quot;
# but at the end of the data, we have 2 rows we&#39;d like to ignore while parsing
# since they&#39;re not properly delimited
data = &quot;&quot;&quot;
a,b,c
1,2,3
4,5,6
7,8,9
totals: 12, 15, 18
grand total: 45
&quot;&quot;&quot;

# by passing `footerskip=2`, we tell parsing to start the end of the data and
# read 2 rows, ignoring their contents, then mark the ending position where
# the normal parsing process should finish
file = CSV.File(IOBuffer(data); footerskip=2)</code></pre><h3 id="transpose_example"><a class="docs-heading-anchor" href="#transpose_example">Reading transposed data</a><a id="transpose_example-1"></a><a class="docs-heading-anchor-permalink" href="#transpose_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# our data is transposed, meaning our column names are in the first column,
# with the data for column &quot;a&quot; all on the first row, data for column &quot;b&quot;
# all on the second row, and so on.
data = &quot;&quot;&quot;
a,1,4,7
b,2,5,8
c,3,6,9
&quot;&quot;&quot;

# by passing `transpose=true`, parsing will look for column names in the first
# column of data, then parse each row as a separate column
file = CSV.File(IOBuffer(data); transpose=true)</code></pre><h3 id="comment_example"><a class="docs-heading-anchor" href="#comment_example">Ignoring commented rows</a><a id="comment_example-1"></a><a class="docs-heading-anchor-permalink" href="#comment_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# here, we have several non-data rows that all beging with the &quot;#&quot; string
data = &quot;&quot;&quot;
# row describing column names
a,b,c
# row describing first row of data
1,2,3
# row describign second row of data
4,5,6
&quot;&quot;&quot;

# we want to ignore these &quot;commented&quot; rows
file = CSV.File(IOBuffer(data); comment=&quot;#&quot;)</code></pre><h3 id="ignoreemptyrows_example"><a class="docs-heading-anchor" href="#ignoreemptyrows_example">Ignoring empty rows</a><a id="ignoreemptyrows_example-1"></a><a class="docs-heading-anchor-permalink" href="#ignoreemptyrows_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# here, we have a &quot;gap&quot; row in between the first and second row of data
# by default, these &quot;empty&quot; rows are ignored, but in our case, this is
# how a row of data is input when all columns have missing/null values
# so we don&#39;t want those rows to be ignored so we can know how many
# missing cases there are in our data
data = &quot;&quot;&quot;
a,b,c
1,2,3

4,5,6
&quot;&quot;&quot;

# by passing `ignoreemptyrows=false`, we ensure parsing treats an empty row
# as each column having a `missing` value set for that row
file = CSV.File(IOBuffer(data); ignoreemptyrows=true)</code></pre><h3 id="select_example"><a class="docs-heading-anchor" href="#select_example">Including/excluding columns</a><a id="select_example-1"></a><a class="docs-heading-anchor-permalink" href="#select_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# simple dataset, but we know column &quot;b&quot; isn&#39;t needed
# so we&#39;d like to save time by having parsing ignore it completely
data = &quot;&quot;&quot;
a,b,c
1,2,3
4,5,6
7,8,9
&quot;&quot;&quot;

# there are quite a few ways to provide the select/drop arguments
# so we provide an example of each, first for selecting the columns
# &quot;a&quot; and &quot;c&quot; that we want to include or keep from parsing
file = CSV.File(file; select=[1, 3])
file = CSV.File(file; select=[:a, :c])
file = CSV.File(file; select=[&quot;a&quot;, &quot;c&quot;])
file = CSV.File(file; select=[true, false, true])
file = CSV.File(file; select=(i, nm) -&gt; i in (1, 3))
# now examples of dropping, when we&#39;d rather specify the column(s)
# we&#39;d like to drop/exclude from parsing
file = CSV.File(file; drop=[2])
file = CSV.File(file; drop=[:b])
file = CSV.File(file; drop=[&quot;b&quot;])
file = CSV.File(file; drop=[false, true, false])
file = CSV.File(file; drop=(i, nm) -&gt; i == 2)</code></pre><h3 id="limit_example"><a class="docs-heading-anchor" href="#limit_example">Limiting number of rows from data</a><a id="limit_example-1"></a><a class="docs-heading-anchor-permalink" href="#limit_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# here, we have quite a few rows of data (relative to other examples, lol)
# but we know we only need the first 3 for the analysis we need to do
# so instead of spending the time parsing the entire file, we&#39;d like
# to just read the first 3 rows and ignore the rest
data = &quot;&quot;&quot;
a,b,c
1,2,3
4,5,6
7,8,9
10,11,12
13,14,15
&quot;&quot;&quot;

# parsing will start reading rows, and once 3 have been read, it will
# terminate early, avoiding the parsing of the rest of the data entirely
file = CSV.File(IOBuffer(data); limit=3)</code></pre><h3 id="missing_string_example"><a class="docs-heading-anchor" href="#missing_string_example">Specifying custom missing strings</a><a id="missing_string_example-1"></a><a class="docs-heading-anchor-permalink" href="#missing_string_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# in this data, our first column has &quot;missing&quot; values coded with -999
# but our score column has &quot;NA&quot; instead
# we&#39;d like either of those values to show up as `missing` after we parse the data
data = &quot;&quot;&quot;
code,age,score
0,21,3.42
1,42,6.55
-999,81,NA
-999,83,NA
&quot;&quot;&quot;

# by passing missingstring=[&quot;-999&quot;, &quot;NA&quot;], parsing will check each cell if it matches
# either string in order to set the value of the cell to `missing`
file = CSV.File(file; missingstring=[&quot;-999&quot;, &quot;NA&quot;])</code></pre><h3 id="string_delim"><a class="docs-heading-anchor" href="#string_delim">String delimiter</a><a id="string_delim-1"></a><a class="docs-heading-anchor-permalink" href="#string_delim" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# our data has two columns, separated by double colon
# characters (&quot;::&quot;)
data = &quot;&quot;&quot;
col1::col2
1::2
3::4
&quot;&quot;&quot;

# we can pass a single character or string for delim
file = CSV.File(file; delim=&quot;::&quot;)</code></pre><h3 id="ignorerepeated_example"><a class="docs-heading-anchor" href="#ignorerepeated_example">Fixed width files</a><a id="ignorerepeated_example-1"></a><a class="docs-heading-anchor-permalink" href="#ignorerepeated_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# This is an example of &quot;fixed width&quot; data, where each 
# column is the same number of characters away from each 
# other on each row. Fields are &quot;padded&quot; with extra 
# delimiters (in this case `&#39; &#39;`) so that each column is 
# the same number of characters each time
data = &quot;&quot;&quot;
col1    col2 col3
123431  2    3421
2355    346  7543
&quot;&quot;&quot;
# In addition to our `delim`, we can pass 
# `ignorerepeated=true`, which tells parsing that 
#consecutive delimiters should be treated as a single 
# delimiter.
file = CSV.File(file; delim=&#39; &#39;, ignorerepeated=true)</code></pre><h3 id="quoted_example"><a class="docs-heading-anchor" href="#quoted_example">Turning off quoted cell parsing</a><a id="quoted_example-1"></a><a class="docs-heading-anchor-permalink" href="#quoted_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# by default, cells like the 1st column, 2nd row
# will be treated as &quot;quoted&quot; cells, where they start
# and end with the quote character &#39;&quot;&#39;. The quotes will
# be removed from the final parsed value
# we may, however, want the &quot;raw&quot; value and _not_ ignore
# the quote characters in the final value
data = &quot;&quot;&quot;
a,b,c
&quot;hey&quot;,2,3
there,4,5
sailor,6,7
&quot;&quot;&quot;

# we can &quot;turn off&quot; the detection of quoted cells
# by passing `quoted=false`
file = CSV.File(IOBuffer(data); quoted=false)</code></pre><h3 id="quotechar_example"><a class="docs-heading-anchor" href="#quotechar_example">Quoted &amp; escaped fields</a><a id="quotechar_example-1"></a><a class="docs-heading-anchor-permalink" href="#quotechar_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In this data, we have a few &quot;quoted&quot; fields, which means the field&#39;s value starts and ends with `quotechar` (or 
# `openquotechar` and `closequotechar`, respectively). Quoted fields allow the field to contain characters that would otherwise 
# be significant to parsing, such as delimiters or newline characters. When quoted, parsing will ignore these otherwise 
# signficant characters until the closing quote character is found. For quoted fields that need to also include the quote 
# character itself, an escape character is provided to tell parsing to ignore the next character when looking for a close quote 
# character. In the syntax examples, the keyword arguments are passed explicitly, but these also happen to be the default 
# values, so just doing `CSV.File(file)` would result in successful parsing.
data = &quot;&quot;&quot;
col1,col2
&quot;quoted field with a delimiter , inside&quot;,&quot;quoted field that contains a \\n newline and &quot;&quot;inner quotes\&quot;\&quot;\&quot;
unquoted field,unquoted field with &quot;inner quotes&quot;
&quot;&quot;&quot;

file = CSV.File(file; quotechar=&#39;&quot;&#39;, escapechar=&#39;&quot;&#39;)

file = CSV.File(file; openquotechar=&#39;&quot;&#39; closequotechar=&#39;&quot;&#39;, escapechar=&#39;&quot;&#39;)</code></pre><h3 id="dateformat_example"><a class="docs-heading-anchor" href="#dateformat_example">DateFormat</a><a id="dateformat_example-1"></a><a class="docs-heading-anchor-permalink" href="#dateformat_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In this file, our `date` column has dates that are formatted like `yyyy/mm/dd`. We can pass just such a string to the 
# `dateformat` keyword argument to tell parsing to use it when looking for `Date` or `DateTime` columns. Note that currently, 
# only a single `dateformat` string can be passed to parsing, meaning multiple columns with different date formats cannot all 
# be parsed as `Date`/`DateTime`.
data = &quot;&quot;&quot;
code,date
0,2019/01/01
1,2019/01/02
&quot;&quot;&quot;

file = CSV.File(file; dateformat=&quot;yyyy/mm/dd&quot;)</code></pre><h3 id="decimal_example"><a class="docs-heading-anchor" href="#decimal_example">Custom decimal separator</a><a id="decimal_example-1"></a><a class="docs-heading-anchor-permalink" href="#decimal_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In many places in the world, floating point number decimals are separated with a comma instead of a period (`3,14` vs. `3.14`)
# . We can correctly parse these numbers by passing in the `decimal=&#39;,&#39;` keyword argument. Note that we probably need to 
# explicitly pass `delim=&#39;;&#39;` in this case, since the parser will probably think that it detected `&#39;,&#39;` as the delimiter.
data = &quot;&quot;&quot;
col1;col2;col3
1,01;2,02;3,03
4,04;5,05;6,06
&quot;&quot;&quot;

file = CSV.File(file; delim=&#39;;&#39;, decimal=&#39;,&#39;)</code></pre><h3 id="truestrings_example"><a class="docs-heading-anchor" href="#truestrings_example">Custom bool strings</a><a id="truestrings_example-1"></a><a class="docs-heading-anchor-permalink" href="#truestrings_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# By default, parsing only considers the string values `true` and `false` as valid `Bool` values. To consider alternative 
# values, we can pass a `Vector{String}` to the `truestrings` and `falsestrings` keyword arguments.
data = &quot;&quot;&quot;
id,paid,attended
0,T,TRUE
1,F,TRUE
2,T,FALSE
3,F,FALSE
&quot;&quot;&quot;

file = CSV.File(file; truestrings=[&quot;T&quot;, &quot;TRUE&quot;], falsestrings=[&quot;F&quot;, &quot;FALSE&quot;])</code></pre><h3 id="matrix_example"><a class="docs-heading-anchor" href="#matrix_example">Matrix-like Data</a><a id="matrix_example-1"></a><a class="docs-heading-anchor-permalink" href="#matrix_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# This file contains a 3x3 identity matrix of `Float64`. By default, parsing will detect the delimiter and type, but we can 
# also explicitly pass `delim= &#39; &#39;` and `types=Float64`, which tells parsing to explicitly treat each column as `Float64`, 
# without having to guess the type on its own.
data = &quot;&quot;&quot;
1.0 0.0 0.0
0.0 1.0 0.0
0.0 0.0 1.0
&quot;&quot;&quot;

file = CSV.File(file; header=false)
file = CSV.File(file; header=false, delim=&#39; &#39;, types=Float64)</code></pre><h3 id="types_example"><a class="docs-heading-anchor" href="#types_example">Providing types</a><a id="types_example-1"></a><a class="docs-heading-anchor-permalink" href="#types_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In this file, our 3rd column has an invalid value on the 2nd row `invalid`. Let&#39;s imagine we&#39;d still like to treat it as an 
# `Int` column, and ignore the `invalid` value. The syntax examples provide several ways we can tell parsing to treat the 3rd 
# column as `Int`, by referring to column index `3`, or column name with `Symbol` or `String`. We can also provide an entire 
# `Vector` of types for each column (and which needs to match the length of columns in the file). There are two additional 
# keyword arguments that control parsing behavior; in the first 4 syntax examples, we would see a warning printed like 
# `&quot;warning: invalid Int64 value on row 2, column 3&quot;`. In the fifth example, passing `silencewarnings=true` will suppress this 
# warning printing. In the last syntax example, passing `strict=true` will result in an error being thrown during parsing.
data = &quot;&quot;&quot;
col1,col2,col3
1,2,3
4,5,invalid
6,7,8
&quot;&quot;&quot;

file = CSV.File(file; types=Dict(3 =&gt; Int))
file = CSV.File(file; types=Dict(:col3 =&gt; Int))
file = CSV.File(file; types=Dict(&quot;col3&quot; =&gt; Int))
file = CSV.File(file; types=[Int, Int, Int])
file = CSV.File(file; types=[Int, Int, Int], silencewarnings=true)
file = CSV.File(file; types=[Int, Int, Int], strict=true)</code></pre><h3 id="typemap_example"><a class="docs-heading-anchor" href="#typemap_example">Typemap</a><a id="typemap_example-1"></a><a class="docs-heading-anchor-permalink" href="#typemap_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In this file, we have U.S. zipcodes in the first column that we&#39;d rather not treat as `Int`, but parsing will detect it as 
# such. In the first syntax example, we pass `typemap=Dict(Int =&gt; String)`, which tells parsing to treat any detected `Int` 
# columns as `String` instead. In the second syntax example, we alternatively set the `zipcode` column type manually.
data = &quot;&quot;&quot;
zipcode,score
03494,9.9
12345,6.7
84044,3.4
&quot;&quot;&quot;

file = CSV.File(file; typemap=Dict(Int =&gt; String))
file = CSV.File(file; types=Dict(:zipcode =&gt; String))</code></pre><h3 id="pool_example"><a class="docs-heading-anchor" href="#pool_example">Pooled values</a><a id="pool_example-1"></a><a class="docs-heading-anchor-permalink" href="#pool_example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CSV

# In this file, we have an `id` column and a `code` column. There can be advantages with various DataFrame/table operations 
# like joining and grouping when `String` values are &quot;pooled&quot;, meaning each unique value is mapped to a `UInt64`. By default, 
# `pool=0.1`, so string columns with low cardinality are pooled by default. Via the `pool` keyword argument, we can provide 
# greater control: `pool=0.4` means that if 40% or less of a column&#39;s values are unique, then it will be pooled.
data = &quot;&quot;&quot;
id,code
A18E9,AT
BF392,GC
93EBC,AT
54EE1,AT
8CD2E,GC
&quot;&quot;&quot;

file = CSV.File(file)
file = CSV.File(file; pool=0.4)
file = CSV.File(file; pool=0.6)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.5 on <span class="colophon-date" title="Friday 20 August 2021 04:37">Friday 20 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
